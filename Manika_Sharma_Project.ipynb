{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Talk Data - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to working with the various data sets in [Wikipedia\n",
    "Talk](https://figshare.com/projects/Wikipedia_Talk/16731) project on Figshare. The release includes:\n",
    "\n",
    "1. a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "2. a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "3. a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "Please refer to our [wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) for documentation of the schema of each data set and our [research paper](https://arxiv.org/abs/1610.08914) for documentation on the data collection and modeling methodology. \n",
    "\n",
    "In this notebook we show how to build a simple classifier for detecting personal attacks and apply the classifier to a random sample of the comment corpus to see whether discussions on user pages have more personal attacks than discussion on article pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, nltk\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37675</th>\n",
       "      <td>`- This is not ``creative``.  Those are the di...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44816</th>\n",
       "      <td>`  :: the term ``standard model`` is itself le...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49851</th>\n",
       "      <td>True or false, the situation as of March 200...</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89320</th>\n",
       "      <td>Next, maybe you could work on being less cond...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93890</th>\n",
       "      <td>This page will need disambiguation.</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  year  logged_in  \\\n",
       "rev_id                                                                       \n",
       "37675   `- This is not ``creative``.  Those are the di...  2002      False   \n",
       "44816   `  :: the term ``standard model`` is itself le...  2002      False   \n",
       "49851     True or false, the situation as of March 200...  2002      False   \n",
       "89320    Next, maybe you could work on being less cond...  2002       True   \n",
       "93890                This page will need disambiguation.   2002       True   \n",
       "\n",
       "             ns  sample  split  attack  \n",
       "rev_id                                  \n",
       "37675   article  random  train   False  \n",
       "44816   article  random  train   False  \n",
       "49851   article  random  train   False  \n",
       "89320   article  random    dev   False  \n",
       "93890   article  random  train   False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "#remove all the punctuations\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"/[^a-z \\d \\' \\. \\, \\\" \\: \\; \\? \\- \\!  \\s]/i\", \" \"))\n",
    "#remove all the hyperlinks\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(r\"http\\S+\", \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"[0-9]\", \"\"))\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279             Iraq is not good  ===  ===  USA is bad   \n",
       "2702703      ____ fuck off you little asshole. If you wan...\n",
       "4632658         i have a dick, its bigger than yours! hahaha\n",
       "6545332      == renault ==  you sad little bpy for drivin...\n",
       "6545351      == renault ==  you sad little bo for driving...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.99      0.97     20422\n",
      "       True       0.92      0.58      0.71      2756\n",
      "\n",
      "avg / total       0.94      0.94      0.94     23178\n",
      "\n",
      "Test ROC AUC: 0.962\n",
      "precision, recall, fscore:  (array([0.94549354, 0.91681109]), array([0.99294878, 0.57583454]), array([0.96864028, 0.70737687]), array([20422,  2756], dtype=int64))\n",
      "confusion matrix:  [[20278   144]\n",
      " [ 1169  1587]]\n"
     ]
    }
   ],
   "source": [
    "#FINAL SUBMITTED ONE\n",
    "# fit a simple text classifier\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "\n",
    "#English Stemmer to stem words to basic form.\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "#analyzer an instance to build analyzer for countvectorizer\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "#method for getting stem words\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "#param grid to be used for tuning the C parameter using GridSearchCV\n",
    "#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "clf = Pipeline([\n",
    "    #Feature Extraction Models-Tfidfvectorizer and countvectorizer\n",
    "    #('vect', TfidfVectorizer(max_df=1.0, min_df=1, analyzer=stemmed_words)),\n",
    "    ('vect', CountVectorizer(max_df = 1.0, min_df=1, max_features=10000, ngram_range=(1,4), \n",
    "                             analyzer=stemmed_words, strip_accents={'ascii'})),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    #('clf', LinearSVC(random_state=0)),\n",
    "    ('clf', LogisticRegression(penalty='l2', dual=False)),\n",
    "    #('clf',GridSearchCV(LogisticRegression(penalty='l2'), param_grid))\n",
    "    #('clf', RandomForestClassifier(n_estimators=200,max_depth=, random_state=1)),\n",
    "    #('clf', MLPClassifier()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "predicted_values = clf.predict(test_comments['comment'])\n",
    "precision_recall_fscore = precision_recall_fscore_support(test_comments['attack'], predicted_values)\n",
    "cmat = confusion_matrix(test_comments['attack'], predicted_values)\n",
    "print(classification_report(test_comments['attack'], predicted_values))\n",
    "             \n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "print('precision, recall, fscore: ', precision_recall_fscore)\n",
    "print('confusion matrix: ', cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.93      1.00      0.96     20422\n",
      "       True       0.95      0.46      0.62      2756\n",
      "\n",
      "avg / total       0.93      0.93      0.92     23178\n",
      "\n",
      "Test ROC AUC: 0.957\n",
      "precision, recall, fscore:  (array([0.93137524, 0.95151515]), array([0.99686612, 0.45573295]), array([0.96300851, 0.61629048]), array([20422,  2756], dtype=int64))\n",
      "confusion matrix:  [[20358    64]\n",
      " [ 1500  1256]]\n"
     ]
    }
   ],
   "source": [
    "#TRIED OUT RANDOM CLASSIFIER. I TRIED OUT WITH MANY DIFFERENT HYPERPARAMETERS as stated in readme. BUT INCLUDING JUST THE ONE \n",
    "#WHICH GAVE THE BEST RESULT (that was 0.957)\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "\n",
    "#English Stemmer to stem words to basic form.\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "#analyzer an instance to build analyzer for countvectorizer\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "#method for getting stem words\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "#param grid to be used for tuning the C parameter using GridSearchCV\n",
    "#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "clf = Pipeline([\n",
    "    #Feature Extraction Models-Tfidfvectorizer and countvectorizer\n",
    "    #('vect', TfidfVectorizer(max_df=1.0, min_df=1, analyzer=stemmed_words)),\n",
    "    ('vect', CountVectorizer(max_df = 1.0, min_df=1, max_features=10000, ngram_range=(1,4), \n",
    "                             analyzer=stemmed_words, strip_accents={'ascii'})),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    #('clf', LogisticRegression(penalty='l2', dual=False)),\n",
    "    #('clf',GridSearchCV(LogisticRegression(penalty='l2'), param_grid))\n",
    "    ('clf', RandomForestClassifier(n_estimators=200,max_depth=100, random_state=1)),\n",
    "    #('clf', MLPClassifier()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict_proba(test_comments['comment'])[:, 1])\n",
    "predicted_values = clf.predict(test_comments['comment'])\n",
    "precision_recall_fscore = precision_recall_fscore_support(test_comments['attack'], predicted_values)\n",
    "cmat = confusion_matrix(test_comments['attack'], predicted_values)\n",
    "print(classification_report(test_comments['attack'], predicted_values))\n",
    "             \n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "print('precision, recall, fscore: ', precision_recall_fscore)\n",
    "print('confusion matrix: ', cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.99      0.97     20422\n",
      "       True       0.88      0.65      0.74      2756\n",
      "\n",
      "avg / total       0.94      0.95      0.94     23178\n",
      "\n",
      "Test ROC AUC: 0.817\n",
      "precision, recall, fscore:  (array([0.95397569, 0.87530682]), array([0.98756243, 0.6469521 ]), array([0.97047855, 0.74400167]), array([20422,  2756], dtype=int64))\n",
      "confusion matrix:  [[20168   254]\n",
      " [  973  1783]]\n"
     ]
    }
   ],
   "source": [
    "#TRIED OUT Linear SVC. ROC Score=0.817\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "\n",
    "#English Stemmer to stem words to basic form.\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "#analyzer an instance to build analyzer for countvectorizer\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "#method for getting stem words\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "#param grid to be used for tuning the C parameter using GridSearchCV\n",
    "#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "clf = Pipeline([\n",
    "    #Feature Extraction Models-Tfidfvectorizer and countvectorizer\n",
    "    #('vect', TfidfVectorizer(max_df=1.0, min_df=1, analyzer=stemmed_words)),\n",
    "    ('vect', CountVectorizer(max_df = 1.0, min_df=1, max_features=10000, ngram_range=(1,4), \n",
    "                             analyzer=stemmed_words, strip_accents={'ascii'})),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    #('clf', LogisticRegression(penalty='l2', dual=False)),\n",
    "    #('clf',GridSearchCV(LogisticRegression(penalty='l2'), param_grid))\n",
    "    #('clf', RandomForestClassifier(n_estimators=200,max_depth=100, random_state=1)),\n",
    "    ('clf', svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
    "     verbose=0))\n",
    "    #('clf', MLPClassifier()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict(test_comments['comment']))\n",
    "predicted_values = clf.predict(test_comments['comment'])\n",
    "precision_recall_fscore = precision_recall_fscore_support(test_comments['attack'], predicted_values)\n",
    "cmat = confusion_matrix(test_comments['attack'], predicted_values)\n",
    "print(classification_report(test_comments['attack'], predicted_values))\n",
    "             \n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "print('precision, recall, fscore: ', precision_recall_fscore)\n",
    "print('confusion matrix: ', cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.93      1.00      0.96     20422\n",
      "       True       0.94      0.44      0.60      2756\n",
      "\n",
      "avg / total       0.93      0.93      0.92     23178\n",
      "\n",
      "Test ROC AUC: 0.718\n",
      "precision, recall, fscore:  (array([0.92934137, 0.94158879]), array([0.99632749, 0.43867925]), array([0.96166934, 0.59851485]), array([20422,  2756], dtype=int64))\n",
      "confusion matrix:  [[20347    75]\n",
      " [ 1547  1209]]\n"
     ]
    }
   ],
   "source": [
    "#TRIED OUT Multinomial NB. ROC score=0.718\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "\n",
    "#English Stemmer to stem words to basic form.\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "#analyzer an instance to build analyzer for countvectorizer\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "#method for getting stem words\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "#param grid to be used for tuning the C parameter using GridSearchCV\n",
    "#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "clf = Pipeline([\n",
    "    #Feature Extraction Models-Tfidfvectorizer and countvectorizer\n",
    "    #('vect', TfidfVectorizer(max_df=1.0, min_df=1, analyzer=stemmed_words)),\n",
    "    ('vect', CountVectorizer(max_df = 1.0, min_df=1, max_features=10000, ngram_range=(1,4), \n",
    "                             analyzer=stemmed_words, strip_accents={'ascii'})),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    #('clf', LogisticRegression(penalty='l2', dual=False)),\n",
    "    #('clf',GridSearchCV(LogisticRegression(penalty='l2'), param_grid))\n",
    "    #('clf', RandomForestClassifier(n_estimators=200,max_depth=100, random_state=1)),\n",
    "    #('clf', svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "    # intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     #multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
    "     #verbose=0))\n",
    "    ('clf', MultinomialNB()),\n",
    "    #('clf', MLPClassifier()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict(test_comments['comment']))\n",
    "predicted_values = clf.predict(test_comments['comment'])\n",
    "precision_recall_fscore = precision_recall_fscore_support(test_comments['attack'], predicted_values)\n",
    "cmat = confusion_matrix(test_comments['attack'], predicted_values)\n",
    "print(classification_report(test_comments['attack'], predicted_values))\n",
    "             \n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "print('precision, recall, fscore: ', precision_recall_fscore)\n",
    "print('confusion matrix: ', cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.95      0.97      0.96     20422\n",
      "       True       0.75      0.66      0.70      2756\n",
      "\n",
      "avg / total       0.93      0.93      0.93     23178\n",
      "\n",
      "Test ROC AUC: 0.815\n",
      "precision, recall, fscore:  (array([0.95469427, 0.75373754]), array([0.97096269, 0.65856313]), array([0.96275976, 0.70294345]), array([20422,  2756], dtype=int64))\n",
      "confusion matrix:  [[19829   593]\n",
      " [  941  1815]]\n"
     ]
    }
   ],
   "source": [
    "#TRIED OUT MLP Classifier. ROC SCORE=0.815\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "\n",
    "#English Stemmer to stem words to basic form.\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "\n",
    "stemmer = EnglishStemmer()\n",
    "#analyzer an instance to build analyzer for countvectorizer\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "#method for getting stem words\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "#param grid to be used for tuning the C parameter using GridSearchCV\n",
    "#param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "\n",
    "clf = Pipeline([\n",
    "    #Feature Extraction Models-Tfidfvectorizer and countvectorizer\n",
    "    #('vect', TfidfVectorizer(max_df=1.0, min_df=1, analyzer=stemmed_words)),\n",
    "    ('vect', CountVectorizer(max_df = 1.0, min_df=1, max_features=10000, ngram_range=(1,4), \n",
    "                             analyzer=stemmed_words, strip_accents={'ascii'})),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    #('clf', LogisticRegression(penalty='l2', dual=False)),\n",
    "    #('clf',GridSearchCV(LogisticRegression(penalty='l2'), param_grid))\n",
    "    #('clf', RandomForestClassifier(n_estimators=200,max_depth=100, random_state=1)),\n",
    "    #('clf', svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     #intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     #multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
    "     #verbose=0))\n",
    "    ('clf', MLPClassifier()),\n",
    "])\n",
    "\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "auc = roc_auc_score(test_comments['attack'], clf.predict(test_comments['comment']))\n",
    "predicted_values = clf.predict(test_comments['comment'])\n",
    "precision_recall_fscore = precision_recall_fscore_support(test_comments['attack'], predicted_values)\n",
    "cmat = confusion_matrix(test_comments['attack'], predicted_values)\n",
    "print(classification_report(test_comments['attack'], predicted_values))\n",
    "             \n",
    "print('Test ROC AUC: %.3f' %auc)\n",
    "print('precision, recall, fscore: ', precision_recall_fscore)\n",
    "print('confusion matrix: ', cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Questions and Answers\n",
    "1. What are the text cleaning methods you tried? What are the ones you have included in the final code?\n",
    "Ans:\n",
    "I did text cleaning to remove any numbers from file, all the hyperlinks from the comments file, digits from the file, I removed the stop words from the comments file. Out of all other methods, the last method proved out to be the most useful and impactful.\n",
    "\n",
    "2. What are the features you considered using ? What features did you use in the final code?\n",
    "Ans: I worked with both TfIdfVectorizer and CountVectorizer but CountVectorizer was giving better results , thus I tried using CountVectorizer with different parameters, max_df, min_df, and by using analyser to use English stemmer , and also strip_accents{“ascii”} to take just ascii values\n",
    "3.  What optimizations did you add in your code ? \n",
    "Ans: I performed data cleaning by removing any numbers, hyperlinks, new lines, tabs. I also tried to perform stemming using English Stemmer. That really improved my results. \n",
    "Once I got good results with Logistic Regression, I tried tuning Logistic Regression’s hyperparameters - C, Dual.  Also Tried to get the optimum value of C using GridSearchCV. \n",
    "4. What are the ML methods you tried out, and what were your best results with each method? Which was the best ML method you saw before tuning hyperparameters?\n",
    "Ans: I tried out MultinomialNB, SVM, LinearSVC, Logistic Regression, Random Forest Classifier, Multiperceptron\n",
    "MultinomialNB: 0.932\n",
    "SVM: 0.942\n",
    "Random Forest: 0.726(maxdepth = 2)\n",
    "0.813(Maxdepth =5)\n",
    "0.833(Maxdepth=8)\n",
    "0.874(n_estimators=20)\n",
    "0.879(n_estimators=25, max_depth=10)\n",
    "0.924(n_estimators=25, max_depth=20)\n",
    "0.934(n_estimators=80, max_depth=30)\n",
    "0.942(n_estimators=100, max_depth=50)\n",
    "0.952(n_estimators=200, max_depth=100)\n",
    "\n",
    "\n",
    "MLPClassifier:\n",
    "Default =0.500\n",
    "Logistic Regression (with tfidfvectoriser): 0.954\n",
    "Logistic Regression(with countvectorizer): 0.957\n",
    "Logistic Regression(With countvectorizer and max_features=100000) : 0.959\n",
    "\n",
    "I also tried using GridSearchCV with Logistic Regression, but It decreased my score from 0.962 to just 0.957\n",
    "\n",
    "The ML method that gave the best results was Logistic Regression. \n",
    "\n",
    "\n",
    "\n",
    "5. What hyperparameter tuning did you do ?\n",
    "I tried tweaking the C value of  0.1, 100, 1000 But all of this declined the roc score from 0.962. \n",
    "I also tried using GridSearchCV. But that also didn’t help in increasing the score.\n",
    "When I included the hyperparameters dual=True and penalty=’l2’ , Even tried using dual =False, \n",
    "But it just doesnot increase score at all. It was stuck at 0.962.  \n",
    "\n",
    "\n",
    "6. What did you learn from the different metrics? Did you try cross-validation?\n",
    "Ans: precision and recall values shows the measure of relevance of the system. And support helped me know about the number of samples of true response that lies in that class. \n",
    "\n",
    "7. What are your best final Result Metrics? By how much is it better than the strawman figure? Which model gave you this performance?\n",
    "Ans: My best result was ruc score 0.962. Which is 0.005 better than strawman figure. Logistic regression used with snowball stemmer and countvectorizer where max features are 1000000 gave me that result. \n",
    "\n",
    "8. What was the hardest thing to do ?\n",
    "Ans: The hardest thing was having no background about Machine learning and thus struggling to figure out the ML Algorithms and understanding what it does was really challenging. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prevalence of personal attacks by namespace\n",
    "In this section we use our classifier in conjunction with the [Wikipedia Talk Corpus](https://figshare.com/articles/Wikipedia_Talk_Corpus/4264973) to see if personal attacks are more common on user talk or article talk page discussions. In our paper we show that the model is not biased by namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from scipy.stats import bernoulli\n",
    "% matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and untar data\n",
    "\n",
    "USER_TALK_CORPUS_2004_URL = 'https://ndownloader.figshare.com/files/6982061'\n",
    "ARTICLE_TALK_CORPUS_2004_URL = 'https://ndownloader.figshare.com/files/7038050'\n",
    "\n",
    "download_file(USER_TALK_CORPUS_2004_URL, 'comments_user_2004.tar.gz')\n",
    "download_file(ARTICLE_TALK_CORPUS_2004_URL,  'comments_article_2004.tar.gz')\n",
    "\n",
    "os.system('tar -xzf comments_user_2004.tar.gz')\n",
    "os.system('tar -xzf comments_article_2004.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for collecting a sample of comments for a given ns and year from \n",
    "def load_no_bot_no_admin(ns, year, prob = 0.1):\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    data_dir = \"comments_%s_%d\" % (ns, year)\n",
    "    for _, _, filenames in os.walk(data_dir):\n",
    "        for filename in filenames:\n",
    "            if re.match(\"chunk_\\d*.tsv\", filename):\n",
    "                df = pd.read_csv(os.path.join(data_dir, filename), sep = \"\\t\")\n",
    "                df['include'] = bernoulli.rvs(prob, size=df.shape[0])\n",
    "                df = df.query(\"bot == 0 and admin == 0 and include == 1\")\n",
    "                dfs.append(df)\n",
    "                \n",
    "    sample = pd.concat(dfs)\n",
    "    sample['ns'] = ns\n",
    "    sample['year'] = year\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a random sample of comments from 2004 for each namespace\n",
    "corpus_user = load_no_bot_no_admin('user', 2004)\n",
    "corpus_article = load_no_bot_no_admin('article', 2004)\n",
    "corpus = pd.concat([corpus_user, corpus_article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model\n",
    "corpus['comment'] = corpus['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "corpus['comment'] = corpus['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "corpus['attack'] = clf.predict_proba(corpus['comment'])[:,1] > 0.425 # see paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prevalence per ns\n",
    "\n",
    "sns.pointplot(data = corpus, x = 'ns', y = 'attack')\n",
    "plt.ylabel(\"Attack fraction\")\n",
    "plt.xlabel(\"Dicussion namespace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attacks are far more prevalent in the user talk namespace."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
